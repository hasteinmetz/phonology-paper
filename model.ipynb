{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = './trainingdata_stepwise_turkish.tsv'\n",
    "model_load_file = './saved_models/gestnet_8_200.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from dev import *\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data_stepwise = Dataset(dataset_file)\n",
    "model = Seq2Seq(training_data=data_stepwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new model or load a previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 13/200 [00:16<03:54,  1.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hillel/Documents/UW/sp2022/phonology/paper/gestnet-master/model.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hillel/Documents/UW/sp2022/phonology/paper/gestnet-master/model.ipynb#ch0000006?line=6'>7</a>\u001b[0m \u001b[39m# create a new model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hillel/Documents/UW/sp2022/phonology/paper/gestnet-master/model.ipynb#ch0000006?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m model_load_file \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hillel/Documents/UW/sp2022/phonology/paper/gestnet-master/model.ipynb#ch0000006?line=8'>9</a>\u001b[0m     model\u001b[39m.\u001b[39;49mtrain_model(training_data\u001b[39m=\u001b[39;49mdata_stepwise, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/UW/sp2022/phonology/paper/gestnet-master/dev.py:356\u001b[0m, in \u001b[0;36mSeq2Seq.train_model\u001b[0;34m(self, training_data, n_epochs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/hillel/Documents/UW/sp2022/phonology/paper/gestnet-master/dev.py?line=352'>353</a>\u001b[0m     predicted_masked \u001b[39m=\u001b[39m mask \u001b[39m*\u001b[39m predicted\n\u001b[1;32m    <a href='file:///Users/hillel/Documents/UW/sp2022/phonology/paper/gestnet-master/dev.py?line=354'>355</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function(predicted_masked\u001b[39m.\u001b[39mfloat(), target\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m--> <a href='file:///Users/hillel/Documents/UW/sp2022/phonology/paper/gestnet-master/dev.py?line=355'>356</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='file:///Users/hillel/Documents/UW/sp2022/phonology/paper/gestnet-master/dev.py?line=356'>357</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='file:///Users/hillel/Documents/UW/sp2022/phonology/paper/gestnet-master/dev.py?line=358'>359</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_model(training_data, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py?line=212'>213</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Tensor \u001b[39mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py?line=213'>214</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py?line=214'>215</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py?line=215'>216</a>\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py?line=218'>219</a>\u001b[0m         retain_graph\u001b[39m=\u001b[39mretain_graph,\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py?line=219'>220</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph)\n\u001b[0;32m--> <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py?line=220'>221</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=126'>127</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=127'>128</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=129'>130</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=130'>131</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph,\n\u001b[1;32m    <a href='file:///Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=131'>132</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load a previous model\n",
    "if model_load_file != 'none':\n",
    "    try:\n",
    "        model = Seq2Seq(load=model_load_file)\n",
    "    except:\n",
    "        model.train_model(training_data=data_stepwise, n_epochs=200)\n",
    "# create a new model\n",
    "if model_load_file == 'none':\n",
    "    model.train_model(training_data=data_stepwise, n_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on all 2 syllable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6  \\\n",
      "0   0.842151  0.685471  0.676226  0.525379  0.021649  0.384931  0.033987   \n",
      "1   0.897407  0.956952  0.949624  0.901668  0.419069  0.798932  0.198599   \n",
      "2   0.852523  0.772747  0.761797  0.633111  0.073592  0.529568  0.082031   \n",
      "3   0.621409  0.543160  0.592652  0.618745  0.402602  0.404765  0.177946   \n",
      "4   0.657484  0.459738  0.548320  0.601951  0.266484  0.297078  0.109275   \n",
      "5   0.610877  0.418875  0.506909  0.528937  0.186519  0.240244  0.083049   \n",
      "6   0.682967  0.590527  0.649539  0.657843  0.316262  0.335354  0.111387   \n",
      "7   0.942226  0.835873  0.771145  0.697956  0.033406  0.517991  0.066218   \n",
      "8   0.853852  0.645232  0.630860  0.486186  0.021859  0.425644  0.025895   \n",
      "9   0.929844  0.942736  0.942647  0.906243  0.317740  0.673207  0.051936   \n",
      "10  0.871312  0.744662  0.733025  0.614336  0.070109  0.542739  0.056347   \n",
      "11  0.620709  0.525611  0.577177  0.615275  0.345125  0.322897  0.036263   \n",
      "12  0.654035  0.436262  0.519390  0.592128  0.236402  0.237819  0.019874   \n",
      "13  0.616054  0.408342  0.485807  0.517436  0.180816  0.262812  0.042663   \n",
      "14  0.692641  0.583970  0.639411  0.653405  0.299388  0.331503  0.049986   \n",
      "15  0.943331  0.788951  0.722830  0.671342  0.029350  0.483788  0.020317   \n",
      "16  0.883736  0.856700  0.932699  0.865158  0.592925  0.209398  0.185711   \n",
      "17  0.808107  0.939595  0.947762  0.877679  0.511975  0.057595  0.210351   \n",
      "18  0.856449  0.887277  0.929453  0.865875  0.531584  0.105171  0.118985   \n",
      "19  0.712380  0.754059  0.786440  0.781612  0.641005  0.383584  0.482633   \n",
      "20  0.712381  0.708113  0.764351  0.753432  0.582939  0.353330  0.443803   \n",
      "21  0.790822  0.770364  0.844954  0.829963  0.613596  0.317423  0.406317   \n",
      "22  0.772125  0.813609  0.845752  0.838559  0.668317  0.297590  0.457826   \n",
      "23  0.882975  0.850759  0.931311  0.857653  0.509555  0.140303  0.133248   \n",
      "24  0.895985  0.857907  0.930047  0.851149  0.562477  0.073076  0.074946   \n",
      "25  0.867453  0.951950  0.960264  0.874973  0.470689  0.009612  0.037683   \n",
      "26  0.877103  0.893259  0.932921  0.860554  0.498793  0.024917  0.062690   \n",
      "27  0.717303  0.760156  0.791598  0.784785  0.553269  0.044717  0.078438   \n",
      "28  0.710250  0.711343  0.764726  0.752900  0.516079  0.047450  0.060199   \n",
      "29  0.794598  0.778921  0.847269  0.830010  0.574287  0.048792  0.158985   \n",
      "30  0.783605  0.824799  0.853284  0.842982  0.617715  0.034559  0.193652   \n",
      "31  0.886474  0.848194  0.925579  0.844058  0.495214  0.032838  0.034597   \n",
      "32  0.789666  0.542505  0.688611  0.641681  0.390152  0.114382  0.127821   \n",
      "33  0.809055  0.777262  0.830052  0.847094  0.652367  0.109883  0.098478   \n",
      "34  0.764927  0.681362  0.758620  0.757733  0.594232  0.192007  0.142474   \n",
      "35  0.590688  0.573578  0.626296  0.634064  0.562175  0.338688  0.351308   \n",
      "36  0.552777  0.497184  0.575321  0.575914  0.481937  0.338011  0.322089   \n",
      "37  0.613101  0.548429  0.618699  0.619089  0.484719  0.337145  0.349991   \n",
      "38  0.642933  0.626697  0.662194  0.676854  0.588038  0.415671  0.395481   \n",
      "39  0.888604  0.760395  0.863445  0.855728  0.605719  0.127399  0.120625   \n",
      "40  0.817316  0.537000  0.676400  0.625811  0.373734  0.034304  0.038421   \n",
      "41  0.853007  0.770272  0.833244  0.851689  0.652308  0.020801  0.008193   \n",
      "42  0.794689  0.680715  0.759173  0.755009  0.586791  0.069618  0.043418   \n",
      "43  0.588452  0.540870  0.611693  0.628557  0.519918  0.053127  0.008935   \n",
      "44  0.550937  0.470889  0.559818  0.568456  0.446671  0.063010  0.009329   \n",
      "45  0.625113  0.555685  0.622839  0.623505  0.462003  0.069320  0.094420   \n",
      "46  0.655767  0.633532  0.672697  0.686651  0.579713  0.085951  0.140421   \n",
      "47  0.887857  0.735516  0.841804  0.849349  0.618904  0.024658  0.007640   \n",
      "\n",
      "           7         8         9 underlying consonant V2 V1  \n",
      "0   0.095806  0.644175  0.843851       ib-H         b  H  i  \n",
      "1   0.127324  0.444881  0.830773       ab-H         b  H  a  \n",
      "2   0.092724  0.571656  0.845393       eb-H         b  H  e  \n",
      "3   0.289751  0.537733  0.647448       ob-H         b  H  o  \n",
      "4   0.309390  0.598600  0.682395       ub-H         b  H  u  \n",
      "5   0.232324  0.517742  0.633796       yb-H         b  H  y  \n",
      "6   0.210549  0.514487  0.693690       Ã¸b-H         b  H  Ã¸  \n",
      "7   0.173562  0.828715  0.956673       É¯b-H         b  H  É¯  \n",
      "8   0.078467  0.707759  0.858340       ib-L         b  L  i  \n",
      "9   0.014908  0.540192  0.899862       ab-L         b  L  a  \n",
      "10  0.073382  0.647913  0.866663       eb-L         b  L  e  \n",
      "11  0.026948  0.425239  0.642221       ob-L         b  L  o  \n",
      "12  0.035188  0.486797  0.676742       ub-L         b  L  u  \n",
      "13  0.120443  0.484528  0.632307       yb-L         b  L  y  \n",
      "14  0.105825  0.493959  0.703060       Ã¸b-L         b  L  Ã¸  \n",
      "15  0.020437  0.817777  0.963495       É¯b-L         b  L  É¯  \n",
      "16  0.238763  0.600571  0.914561       ig-H         g  H  i  \n",
      "17  0.118805  0.339644  0.806677       ag-H         g  H  a  \n",
      "18  0.231177  0.594303  0.883616       eg-H         g  H  e  \n",
      "19  0.352992  0.619030  0.745883       og-H         g  H  o  \n",
      "20  0.363794  0.654185  0.754507       ug-H         g  H  u  \n",
      "21  0.404985  0.693578  0.838547       yg-H         g  H  y  \n",
      "22  0.363808  0.630584  0.796850       Ã¸g-H         g  H  Ã¸  \n",
      "23  0.200207  0.643868  0.932049       É¯g-H         g  H  É¯  \n",
      "24  0.231792  0.648792  0.930143       ig-L         g  L  i  \n",
      "25  0.122121  0.370670  0.851458       ag-L         g  L  a  \n",
      "26  0.187405  0.585519  0.896879       eg-L         g  L  e  \n",
      "27  0.092497  0.448079  0.728171       og-L         g  L  o  \n",
      "28  0.077508  0.483212  0.732315       ug-L         g  L  u  \n",
      "29  0.125749  0.568527  0.818042       yg-L         g  L  y  \n",
      "30  0.113373  0.509559  0.786557       Ã¸g-L         g  L  Ã¸  \n",
      "31  0.128389  0.666377  0.933895       É¯g-L         g  L  É¯  \n",
      "32  0.222185  0.479538  0.743863       id-H         d  H  i  \n",
      "33  0.162108  0.309850  0.615359       ad-H         d  H  a  \n",
      "34  0.187689  0.399926  0.698615       ed-H         d  H  e  \n",
      "35  0.379805  0.544255  0.623712       od-H         d  H  o  \n",
      "36  0.344061  0.505797  0.589883       ud-H         d  H  u  \n",
      "37  0.374862  0.513632  0.635093       yd-H         d  H  y  \n",
      "38  0.400571  0.518644  0.639417       Ã¸d-H         d  H  Ã¸  \n",
      "39  0.175027  0.525961  0.877982       É¯d-H         d  H  É¯  \n",
      "40  0.242107  0.593129  0.803492       id-L         d  L  i  \n",
      "41  0.066571  0.488092  0.719803       ad-L         d  L  a  \n",
      "42  0.191105  0.492980  0.752716       ed-L         d  L  e  \n",
      "43  0.047498  0.415967  0.618236       od-L         d  L  o  \n",
      "44  0.037961  0.361678  0.578829       ud-L         d  L  u  \n",
      "45  0.146588  0.455407  0.652646       yd-L         d  L  y  \n",
      "46  0.154170  0.475689  0.670048       Ã¸d-L         d  L  Ã¸  \n",
      "47  0.064421  0.664383  0.927472       É¯d-L         d  L  É¯  \n"
     ]
    }
   ],
   "source": [
    "# load dataframe\n",
    "data = pd.read_csv(dataset_file, sep='\\t')\n",
    "data = data[data['syllables']==2]\n",
    "\n",
    "# helper function to get decoder outputs\n",
    "def get_decoder(input: torch.Tensor, target: torch.Tensor) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        _, attn_map_seq = model(input, target)\n",
    "    return attn_map_seq.numpy()[:,0] # attention paid to the first letter\n",
    "\n",
    "# helper functions to get correct inputs\n",
    "def get_trial(training_data, word):\n",
    "    trial = training_data.make_trial(word)\n",
    "    return trial[0], torch.cat((trial[1], trial[2]), axis=1)\n",
    "\n",
    "# get the decoder outputs for each word\n",
    "get_out = lambda x, y : pd.DataFrame(get_decoder(*get_trial(x, y)))\n",
    "df = get_out(data_stepwise, data['underlying'].values[0]).T\n",
    "\n",
    "for i in range(1, data['underlying'].shape[0]):\n",
    "    df = pd.concat(\n",
    "        (df, get_out(data_stepwise, data['underlying'].values[i]).T),\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "df = df.reset_index().drop('index', axis=1)\n",
    "\n",
    "# add columns\n",
    "for c in ['underlying', 'consonant', 'vowel']:\n",
    "    col = data[c]\n",
    "    col = col.reset_index().drop('index', axis=1)\n",
    "    df[c] = col\n",
    "\n",
    "df = df.rename({'vowel': \"V2\"}, axis=1)\n",
    "df = df.assign(\n",
    "    V1 = lambda d: d['underlying'].astype(str).str[0]\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create additional categorical values\n",
    "df_a = df.assign(\n",
    "    rounded = lambda d: d[\"V1\"].apply(lambda y: 1 if y in [\"Ã¸\", \"u\", \"y\", \"o\"] else 0)\n",
    ")\n",
    "df_b = df_a.assign(\n",
    "    fronted = lambda d: d[\"V1\"].apply(lambda y: 1 if y in [\"Ã¸\", \"e\", \"y\", \"i\"] else 0)\n",
    ")\n",
    "df_c = df_b.assign(\n",
    "    high = lambda d: d[\"V1\"].apply(lambda y: 1 if y in[\"Ã¸\", \"u\", \"y\", \"o\", \"i\", \"É¯\"] else 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    V1 V2 consonant underlying fronted rounded high  Time  Attention\n",
      "0    i  H         b       ib-H       1       0    1     5   0.384931\n",
      "1    a  H         b       ab-H       0       0    0     5   0.798932\n",
      "2    e  H         b       eb-H       1       0    0     5   0.529568\n",
      "3    o  H         b       ob-H       0       1    1     5   0.404765\n",
      "4    u  H         b       ub-H       0       1    1     5   0.297078\n",
      "..  .. ..       ...        ...     ...     ...  ...   ...        ...\n",
      "235  o  L         d       od-L       0       1    1     9   0.618236\n",
      "236  u  L         d       ud-L       0       1    1     9   0.578829\n",
      "237  y  L         d       yd-L       1       1    1     9   0.652646\n",
      "238  Ã¸  L         d       Ã¸d-L       1       1    1     9   0.670048\n",
      "239  É¯  L         d       É¯d-L       0       0    1     9   0.927472\n",
      "\n",
      "[240 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df_melt = pd.melt(\n",
    "    frame=df_c,\n",
    "    id_vars=[\"V1\", \"V2\", \"consonant\", \"underlying\", \"fronted\", \"rounded\", \"high\"],\n",
    "    value_name=\"Attention\",\n",
    "    value_vars=[5, 6, 7, 8, 9],\n",
    "    var_name=\"Time\"\n",
    ")\n",
    "\n",
    "# set the categories as well\n",
    "df_mle = df_melt.astype(\n",
    "    {\n",
    "        \"Time\": 'int64', \n",
    "        \"V1\": 'category', \n",
    "        \"V2\": 'category', \n",
    "        \"consonant\": 'category', \n",
    "        \"fronted\": 'category', \n",
    "        \"rounded\": 'category', \n",
    "        \"high\": 'category', \n",
    "        \"underlying\": 'category'\n",
    "    }\n",
    ")\n",
    "print(df_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the analysis on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = sm.datasets.get_rdataset(\"dietox\", \"geepack\").data\n",
    "# print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Mixed Linear Model Regression Results\n",
      "=======================================================\n",
      "Model:            MixedLM Dependent Variable: Attention\n",
      "No. Observations: 240     Method:             REML     \n",
      "No. Groups:       48      Scale:              0.0329   \n",
      "Min. group size:  5       Log-Likelihood:     60.4063  \n",
      "Max. group size:  5       Converged:          Yes      \n",
      "Mean group size:  5.0                                  \n",
      "-------------------------------------------------------\n",
      "            Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
      "-------------------------------------------------------\n",
      "Intercept   -0.611    0.060 -10.139 0.000 -0.729 -0.493\n",
      "V2[T.L]     -0.089    0.023  -3.814 0.000 -0.135 -0.043\n",
      "Time         0.147    0.008  17.744 0.000  0.131  0.163\n",
      "Group Var    0.000    0.013                            \n",
      "=======================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/statsmodels/regression/mixed_linear_model.py:1634: UserWarning: Random effects covariance is singular\n",
      "  warnings.warn(msg)\n",
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "''' From paper:\n",
    "We used the identity of input V2 as \n",
    "    - either a high harmony trigger (/i, u/) \n",
    "      or a non-high non-trigger\n",
    "    - and decoder timepoint as main factors, \n",
    "    - model as a random factor, \n",
    "    - and the attention value assigned to the encoder hidden state \n",
    "        associated with input V2 as the dependent variable.\n",
    "\n",
    "This result suggests that the\n",
    "decoder learns to pay more attention to a V2 at an\n",
    "earlier timepoint when that V2 is a harmony trigger,\n",
    "consistent with the representation of an anticipatory\n",
    "(early-activating) gesture assumed by the Gestural\n",
    "Harmony Model.\n",
    "'''\n",
    "\n",
    "md = smf.ols(\"Attention ~ Time + V2 + fronted * high * rounded\", df_mle, groups=df_mle[\"underlying\"])\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>Time</th>\n",
       "      <th>Attention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>5</td>\n",
       "      <td>0.306102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>6</td>\n",
       "      <td>0.217068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>7</td>\n",
       "      <td>0.252178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>8</td>\n",
       "      <td>0.551307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>9</td>\n",
       "      <td>0.759610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>0.167382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L</td>\n",
       "      <td>6</td>\n",
       "      <td>0.056469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L</td>\n",
       "      <td>7</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L</td>\n",
       "      <td>8</td>\n",
       "      <td>0.534676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L</td>\n",
       "      <td>9</td>\n",
       "      <td>0.776808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  V2  Time  Attention\n",
       "0  H     5   0.306102\n",
       "1  H     6   0.217068\n",
       "2  H     7   0.252178\n",
       "3  H     8   0.551307\n",
       "4  H     9   0.759610\n",
       "5  L     5   0.167382\n",
       "6  L     6   0.056469\n",
       "7  L     7   0.104369\n",
       "8  L     8   0.534676\n",
       "9  L     9   0.776808"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get means of V2 groups\n",
    "df_mle.groupby(['V2', 'Time'], as_index=False).agg({'Attention':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>\n",
       "<tr>\n",
       "  <th>group1</th> <th>group2</th> <th>meandiff</th>  <th>p-adj</th>  <th>lower</th>   <th>upper</th>  <th>reject</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>H</td>      <td>L</td>    <td>-0.0893</td> <td>0.0129</td> <td>-0.1596</td> <td>-0.0191</td>  <td>True</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "mc = MultiComparison(df_mle['Attention'], groups=df_mle['V2'])\n",
    "mc.tukeyhsd().summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e86c00212425bd2fc84232e5edf67c3bf816ef44de5709770dab76c6c537ba08"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
