{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = './trainingdata_stepwise_turkish.tsv'\n",
    "model_load_file = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from dev import *\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data_stepwise = Dataset(dataset_file)\n",
    "model = Seq2Seq(training_data=data_stepwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new model or load a previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a previous model\n",
    "if model_load_file != 'none':\n",
    "    model = Seq2Seq(load=model_load_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [00:25<03:37,  1.21s/it]"
     ]
    }
   ],
   "source": [
    "# create a new model\n",
    "if model_load_file == 'none':\n",
    "    model.train_model(training_data=data_stepwise, n_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on all 2 syllable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6  \\\n",
      "0   0.159603  0.159574  0.159588  0.159598  0.159599  0.159599  0.159599   \n",
      "1   0.158703  0.158714  0.158722  0.158729  0.158729  0.158729  0.158729   \n",
      "2   0.159603  0.159574  0.159588  0.159598  0.159599  0.159599  0.159599   \n",
      "3   0.155098  0.155115  0.155114  0.155122  0.155123  0.155123  0.155123   \n",
      "4   0.155098  0.155115  0.155114  0.155122  0.155123  0.155123  0.155123   \n",
      "5   0.157877  0.157833  0.157838  0.157852  0.157851  0.157852  0.157852   \n",
      "6   0.157877  0.157833  0.157838  0.157852  0.157851  0.157852  0.157852   \n",
      "7   0.158703  0.158714  0.158722  0.158729  0.158729  0.158729  0.158729   \n",
      "8   0.169476  0.169465  0.169462  0.169469  0.169469  0.169468  0.169468   \n",
      "9   0.168214  0.168218  0.168216  0.168229  0.168230  0.168228  0.168229   \n",
      "10  0.169476  0.169465  0.169462  0.169469  0.169469  0.169468  0.169468   \n",
      "11  0.168214  0.168218  0.168216  0.168229  0.168230  0.168228  0.168229   \n",
      "12  0.168214  0.168218  0.168216  0.168229  0.168230  0.168228  0.168229   \n",
      "13  0.169476  0.169465  0.169462  0.169469  0.169469  0.169468  0.169468   \n",
      "14  0.169476  0.169465  0.169462  0.169469  0.169469  0.169468  0.169468   \n",
      "15  0.168214  0.168218  0.168216  0.168229  0.168230  0.168228  0.168229   \n",
      "16  0.160044  0.160065  0.160076  0.160085  0.160084  0.160084  0.160084   \n",
      "17  0.159101  0.159149  0.159153  0.159158  0.159157  0.159157  0.159157   \n",
      "18  0.160044  0.160065  0.160076  0.160085  0.160084  0.160084  0.160084   \n",
      "19  0.155255  0.155312  0.155308  0.155317  0.155315  0.155316  0.155316   \n",
      "20  0.155255  0.155312  0.155308  0.155317  0.155315  0.155316  0.155316   \n",
      "21  0.158183  0.158202  0.158210  0.158223  0.158219  0.158220  0.158220   \n",
      "22  0.158183  0.158202  0.158210  0.158223  0.158219  0.158220  0.158220   \n",
      "23  0.159101  0.159149  0.159153  0.159158  0.159157  0.159157  0.159157   \n",
      "24  0.169962  0.169973  0.169971  0.169968  0.169969  0.169968  0.169968   \n",
      "25  0.168744  0.168774  0.168776  0.168777  0.168779  0.168778  0.168778   \n",
      "26  0.169962  0.169973  0.169971  0.169968  0.169969  0.169968  0.169968   \n",
      "27  0.168744  0.168774  0.168776  0.168777  0.168779  0.168778  0.168778   \n",
      "28  0.168744  0.168774  0.168776  0.168777  0.168779  0.168778  0.168778   \n",
      "29  0.169962  0.169973  0.169971  0.169968  0.169969  0.169968  0.169968   \n",
      "30  0.169962  0.169973  0.169971  0.169968  0.169969  0.169968  0.169968   \n",
      "31  0.168744  0.168774  0.168776  0.168777  0.168779  0.168778  0.168778   \n",
      "32  0.152993  0.152992  0.152990  0.153002  0.153004  0.153003  0.153004   \n",
      "33  0.152084  0.152123  0.152114  0.152125  0.152127  0.152126  0.152126   \n",
      "34  0.152993  0.152992  0.152990  0.153002  0.153004  0.153003  0.153004   \n",
      "35  0.148476  0.148530  0.148511  0.148524  0.148526  0.148525  0.148525   \n",
      "36  0.148476  0.148530  0.148511  0.148524  0.148526  0.148525  0.148525   \n",
      "37  0.151256  0.151256  0.151246  0.151263  0.151263  0.151263  0.151263   \n",
      "38  0.151256  0.151256  0.151246  0.151263  0.151263  0.151263  0.151263   \n",
      "39  0.152084  0.152123  0.152114  0.152125  0.152127  0.152126  0.152126   \n",
      "40  0.159711  0.159746  0.159752  0.159754  0.159756  0.159755  0.159755   \n",
      "41  0.158528  0.158587  0.158598  0.158604  0.158608  0.158607  0.158606   \n",
      "42  0.159711  0.159746  0.159752  0.159754  0.159756  0.159755  0.159755   \n",
      "43  0.158528  0.158587  0.158598  0.158604  0.158608  0.158607  0.158606   \n",
      "44  0.158528  0.158587  0.158598  0.158604  0.158608  0.158607  0.158606   \n",
      "45  0.159711  0.159746  0.159752  0.159754  0.159756  0.159755  0.159755   \n",
      "46  0.159711  0.159746  0.159752  0.159754  0.159756  0.159755  0.159755   \n",
      "47  0.158528  0.158587  0.158598  0.158604  0.158608  0.158607  0.158606   \n",
      "\n",
      "           7         8         9 underlying consonant V2 V1  \n",
      "0   0.159599  0.159599  0.159599       ib-H         b  H  i  \n",
      "1   0.158729  0.158729  0.158729       ab-H         b  H  a  \n",
      "2   0.159599  0.159599  0.159599       eb-H         b  H  e  \n",
      "3   0.155123  0.155123  0.155122       ob-H         b  H  o  \n",
      "4   0.155123  0.155123  0.155122       ub-H         b  H  u  \n",
      "5   0.157852  0.157852  0.157852       yb-H         b  H  y  \n",
      "6   0.157852  0.157852  0.157852       øb-H         b  H  ø  \n",
      "7   0.158729  0.158729  0.158729       ɯb-H         b  L  ɯ  \n",
      "8   0.169468  0.169468  0.169468       ib-L         b  L  i  \n",
      "9   0.168229  0.168229  0.168229       ab-L         b  L  a  \n",
      "10  0.169468  0.169468  0.169468       eb-L         b  L  e  \n",
      "11  0.168229  0.168229  0.168229       ob-L         b  L  o  \n",
      "12  0.168229  0.168229  0.168229       ub-L         b  L  u  \n",
      "13  0.169468  0.169468  0.169468       yb-L         b  L  y  \n",
      "14  0.169468  0.169468  0.169468       øb-L         b  L  ø  \n",
      "15  0.168229  0.168229  0.168229       ɯb-L         b  L  ɯ  \n",
      "16  0.160084  0.160084  0.160084       ig-H         g  H  i  \n",
      "17  0.159157  0.159157  0.159157       ag-H         g  H  a  \n",
      "18  0.160084  0.160084  0.160084       eg-H         g  H  e  \n",
      "19  0.155316  0.155316  0.155316       og-H         g  H  o  \n",
      "20  0.155316  0.155316  0.155316       ug-H         g  H  u  \n",
      "21  0.158220  0.158220  0.158220       yg-H         g  H  y  \n",
      "22  0.158220  0.158220  0.158220       øg-H         g  H  ø  \n",
      "23  0.159157  0.159157  0.159157       ɯg-H         g  L  ɯ  \n",
      "24  0.169968  0.169968  0.169968       ig-L         g  L  i  \n",
      "25  0.168778  0.168778  0.168778       ag-L         g  L  a  \n",
      "26  0.169968  0.169968  0.169968       eg-L         g  L  e  \n",
      "27  0.168778  0.168778  0.168778       og-L         g  L  o  \n",
      "28  0.168778  0.168778  0.168778       ug-L         g  L  u  \n",
      "29  0.169968  0.169968  0.169968       yg-L         g  L  y  \n",
      "30  0.169968  0.169968  0.169968       øg-L         g  L  ø  \n",
      "31  0.168778  0.168778  0.168778       ɯg-L         g  L  ɯ  \n",
      "32  0.153004  0.153004  0.153004       id-H         d  H  i  \n",
      "33  0.152126  0.152126  0.152126       ad-H         d  H  a  \n",
      "34  0.153004  0.153004  0.153004       ed-H         d  H  e  \n",
      "35  0.148525  0.148525  0.148525       od-H         d  H  o  \n",
      "36  0.148525  0.148525  0.148525       ud-H         d  H  u  \n",
      "37  0.151263  0.151263  0.151263       yd-H         d  H  y  \n",
      "38  0.151263  0.151263  0.151263       ød-H         d  H  ø  \n",
      "39  0.152126  0.152126  0.152126       ɯd-H         d  L  ɯ  \n",
      "40  0.159755  0.159755  0.159755       id-L         d  L  i  \n",
      "41  0.158607  0.158606  0.158606       ad-L         d  L  a  \n",
      "42  0.159755  0.159755  0.159755       ed-L         d  L  e  \n",
      "43  0.158607  0.158606  0.158606       od-L         d  L  o  \n",
      "44  0.158607  0.158606  0.158606       ud-L         d  L  u  \n",
      "45  0.159755  0.159755  0.159755       yd-L         d  L  y  \n",
      "46  0.159755  0.159755  0.159755       ød-L         d  L  ø  \n",
      "47  0.158607  0.158606  0.158606       ɯd-L         d  L  ɯ  \n"
     ]
    }
   ],
   "source": [
    "# load dataframe\n",
    "data = pd.read_csv(dataset_file, sep='\\t')\n",
    "data = data[data['syllables']==2]\n",
    "\n",
    "# helper function to get decoder outputs\n",
    "def get_decoder(input: torch.Tensor, target: torch.Tensor) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        _, attn_map_seq = model(input, target)\n",
    "    return attn_map_seq.numpy()[:,0] # attention paid to the first letter\n",
    "\n",
    "# helper functions to get correct inputs\n",
    "def get_trial(training_data, word):\n",
    "    trial = training_data.make_trial(word)\n",
    "    return trial[0], torch.cat((trial[1], trial[2]), axis=1)\n",
    "\n",
    "# get the decoder outputs for each word\n",
    "get_out = lambda x, y : pd.DataFrame(get_decoder(*get_trial(x, y)))\n",
    "df = get_out(data_stepwise, data['underlying'].values[0]).T\n",
    "\n",
    "for i in range(1, data['underlying'].shape[0]):\n",
    "    df = pd.concat(\n",
    "        (df, get_out(data_stepwise, data['underlying'].values[i]).T),\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "df = df.reset_index().drop('index', axis=1)\n",
    "\n",
    "# add columns\n",
    "for c in ['underlying', 'consonant', 'vowel']:\n",
    "    col = data[c]\n",
    "    col = col.reset_index().drop('index', axis=1)\n",
    "    df[c] = col\n",
    "\n",
    "df = df.rename({'vowel': \"V2\"}, axis=1)\n",
    "df = df.assign(\n",
    "    V1 = lambda d: d['underlying'].astype(str).str[0]\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create additional categorical values\n",
    "df_a = df.assign(\n",
    "    rounded = lambda d: d[\"V1\"].apply(lambda y: 1 if y in [\"ø\", \"u\", \"y\", \"o\"] else 0)\n",
    ")\n",
    "df_b = df_a.assign(\n",
    "    fronted = lambda d: d[\"V1\"].apply(lambda y: 1 if y in [\"ø\", \"e\", \"y\", \"i\"] else 0)\n",
    ")\n",
    "df_c = df_b.assign(\n",
    "    high = lambda d: d[\"V1\"].apply(lambda y: 1 if y in[\"ø\", \"u\", \"y\", \"o\", \"i\", \"ɯ\"] else 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    V1 V2 consonant underlying fronted rounded high  Time  Attention\n",
      "0    i  H         b       ib-H       1       0    1     5   0.159599\n",
      "1    a  H         b       ab-H       0       0    0     5   0.158729\n",
      "2    e  H         b       eb-H       1       0    0     5   0.159599\n",
      "3    o  H         b       ob-H       0       1    1     5   0.155123\n",
      "4    u  H         b       ub-H       0       1    1     5   0.155123\n",
      "..  .. ..       ...        ...     ...     ...  ...   ...        ...\n",
      "235  o  L         d       od-L       0       1    1     9   0.158606\n",
      "236  u  L         d       ud-L       0       1    1     9   0.158606\n",
      "237  y  L         d       yd-L       1       1    1     9   0.159755\n",
      "238  ø  L         d       ød-L       1       1    1     9   0.159755\n",
      "239  ɯ  L         d       ɯd-L       0       0    1     9   0.158606\n",
      "\n",
      "[240 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df_melt = pd.melt(\n",
    "    frame=df_c,\n",
    "    id_vars=[\"V1\", \"V2\", \"consonant\", \"underlying\", \"fronted\", \"rounded\", \"high\"],\n",
    "    value_name=\"Attention\",\n",
    "    value_vars=[5, 6, 7, 8, 9],\n",
    "    var_name=\"Time\"\n",
    ")\n",
    "\n",
    "# set the categories as well\n",
    "df_mle = df_melt.astype(\n",
    "    {\n",
    "        \"Time\": 'int64', \n",
    "        \"V1\": 'category', \n",
    "        \"V2\": 'category', \n",
    "        \"consonant\": 'category', \n",
    "        \"fronted\": 'category', \n",
    "        \"rounded\": 'category', \n",
    "        \"high\": 'category', \n",
    "        \"underlying\": 'category'\n",
    "    }\n",
    ")\n",
    "print(df_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the analysis on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pig         int64\n",
      "Evit       object\n",
      "Cu         object\n",
      "Litter      int64\n",
      "Start     float64\n",
      "Weight    float64\n",
      "Feed      float64\n",
      "Time        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# data = sm.datasets.get_rdataset(\"dietox\", \"geepack\").data\n",
    "# print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              Attention   R-squared:                       0.540\n",
      "Model:                            OLS   Adj. R-squared:                  0.530\n",
      "Method:                 Least Squares   F-statistic:                     54.94\n",
      "Date:                Tue, 31 May 2022   Prob (F-statistic):           1.35e-37\n",
      "Time:                        14:31:39   Log-Likelihood:                 959.15\n",
      "No. Observations:                 240   AIC:                            -1906.\n",
      "Df Residuals:                     234   BIC:                            -1885.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        0.1552      0.002     96.057      0.000       0.152       0.158\n",
      "rounded[T.1]     0.0011      0.001      1.491      0.137      -0.000       0.003\n",
      "fronted[T.1]     0.0027      0.001      4.678      0.000       0.002       0.004\n",
      "high[T.1]       -0.0025      0.001     -2.933      0.004      -0.004      -0.001\n",
      "V2[T.L]          0.0098      0.001     16.198      0.000       0.009       0.011\n",
      "Time          2.049e-08      0.000   9.97e-05      1.000      -0.000       0.000\n",
      "==============================================================================\n",
      "Omnibus:                       79.204   Durbin-Watson:                   0.458\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               22.615\n",
      "Skew:                          -0.512   Prob(JB):                     1.23e-05\n",
      "Kurtosis:                       1.900   Cond. No.                         41.4\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hillel/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:127: ValueWarning: unknown kwargs ['groups']\n",
      "  warnings.warn(msg, ValueWarning)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "md = smf.ols(\"Attention ~ Time + rounded + fronted + high + V2\", df_mle, groups=df_mle[\"underlying\"])\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>\n",
       "<tr>\n",
       "  <th>group1</th> <th>group2</th> <th>meandiff</th>  <th>p-adj</th>  <th>lower</th>   <th>upper</th> <th>reject</th>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>0</td>      <td>1</td>    <td>0.0015</td>  <td>0.0737</td> <td>-0.0001</td> <td>0.0032</td>  <td>False</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "mc = MultiComparison(df_mle['Attention'], groups=df_mle['fronted'])\n",
    "mc.tukeyhsd().summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e86c00212425bd2fc84232e5edf67c3bf816ef44de5709770dab76c6c537ba08"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
